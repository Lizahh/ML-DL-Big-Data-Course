{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **----------- K Nearest Neighbors (KNN):-----------**\n",
        "\n",
        "* It classify points based on 'distance'\n",
        "\n",
        "* It finds k nearest points based on distance matrix\n",
        "\n",
        "* Let them all vote on the classification\n",
        "\n",
        "That's it.\n",
        "\n"
      ],
      "metadata": {
        "id": "pG-HFF7LFJap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **----------- Dimensionality Reduction:--------------**\n",
        "\n",
        "## **Curse of dimensionality:**\n",
        "\n",
        "* Many problems can have a lot of features\n",
        "\n",
        "* For example, in movie recommendation, the rating vector may represent a dimension --- every movie is its own dimension. Ugh! Tough to visualize.\n",
        "\n",
        "* Dimensionality reduction attempts to distill higher dimensionality data to smaller number of dimensions, while preserving as much variance in the data as possible\n",
        "\n",
        "* We can use dimensionality reduction for visualization, image compression and feature extraction.\n",
        "\n",
        "* K-Means clustering is a dimensionality reduction algorithm. It reduces data down to k clusters.\n",
        "\n",
        "* Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) are closely related mathematical techniques, and PCA can be understood as a specific application of SVD.\n",
        "\n"
      ],
      "metadata": {
        "id": "VAjoqjC4KK1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Singular Value Decomposition (SVD):**\n",
        "\n",
        "1. Imagine you have a big table of numbers, like a spreadsheet. Singular Value Decomposition, or SVD for short, is like a magic trick that breaks down this table into three smaller, simpler tables.\n",
        "\n",
        "**Left Table (U):** This table helps us understand the patterns in the rows of our big table.\n",
        "\n",
        "**Middle Table (Σ):** This table tells us how important these patterns are, listed from the most important to the least important.\n",
        "\n",
        "**Right Table (V):** This table helps us understand the patterns in the columns of our big table.\n",
        "\n",
        "SVD is used in many things like reducing noise in images, understanding text documents, and making recommendations on websites like Netflix. It's a powerful tool to find hidden patterns in data.\n",
        "\n",
        "2. Singular Value Decomposition (SVD) is related to dimensionality reduction in the following way:\n",
        "\n",
        "**SVD Breaks Down Data:** SVD decomposes a large dataset (usually presented as a matrix) into three smaller matrices: U, Σ, and V. These matrices capture the underlying patterns and relationships in the data.\n",
        "\n",
        "**Dimensionality Reduction with SVD:** To perform dimensionality reduction using SVD, you typically focus on the middle matrix, Σ. This matrix contains singular values that represent the importance of different patterns in the data. The singular values are ordered from most important (at the top) to least important (at the bottom).\n",
        "\n",
        "OR\n",
        "\n",
        "When you use SVD to make your data simpler, you look at a special list called Σ. This list tells you which things in your data are really important (at the top of the list) and which things are less important (at the bottom of the list). By focusing on the important things and ignoring the less important ones, you can make your data easier to work with.\n",
        "\n",
        "**Selecting Top Singular Values:** To reduce dimensionality, you keep only the top singular values and their corresponding columns in U and rows in V. By doing this, you retain the most significant patterns in the data while discarding less important ones.\n",
        "\n",
        "OR\n",
        "\n",
        "To make your data simpler, you pick the most important things from the list Σ. These are the things at the top of the list. Then, you use these important things to create a new, shorter list. This new list keeps the most crucial patterns in your data and gets rid of the less important ones, making your data easier to handle.\n",
        "\n",
        "**Reduced Data Representation:** The resulting matrices U, Σ (with only the top singular values), and V provide a lower-dimensional representation of your original data. You can use this reduced representation for various purposes, such as visualization, data compression, or feeding it into other machine learning algorithms.\n",
        "\n",
        "OR\n",
        "\n",
        "After picking the most important things from the list Σ, you end up with three smaller lists: U, Σ (with only the top things), and V. These smaller lists give you a simpler way to understand your data. You can use this simpler representation to do things like make pictures, save space, or make computers learn from the data more easily.\n",
        "\n",
        "# Principal Component Analysis (PCA) is a dimensionality reduction technique that uses Singular Value Decomposition (SVD) as its mathematical foundation. PCA leverages SVD to find and rank the most important patterns (principal components) in data for dimensionality reduction."
      ],
      "metadata": {
        "id": "Euv0L5wmYquD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **---------- Datawarehouse ETL and ELT Processes - Overview:----------**\n",
        "\n",
        "## **A) Monolithic Database:**\n",
        "\n",
        "A monolithic database refers to a **traditional**, **centralized** database architecture where all components, including data storage, management, and processing, are tightly integrated into a **single, unified system**.\n",
        "\n",
        "OR\n",
        "\n",
        "A monolithic database is like a single, all-in-one system where everything related to data, like storing, organizing, and using it, happens in one place, making it less flexible.\n",
        "\n",
        ">> **----Difference in Monolithic and Distributed Database:-----**\n",
        "\n",
        "**Monolithic Database:**\n",
        "\n",
        "* Centralized system (A centralized database is a type of database system where all data is stored, managed, and controlled from a single location or server)\n",
        "* All data and processing on a single server.\n",
        "* Limited scalability.\n",
        "* Typically used in older or smaller systems.\n",
        "\n",
        "**Distributed Database:**\n",
        "\n",
        "* Distributed across multiple servers or nodes.\n",
        "* Enables better scalability and fault tolerance.\n",
        "* Common in modern, large-scale applications.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9DCryqOgfImt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **B) Hadoop Cluster:**\n",
        "\n",
        "A Hadoop cluster is a group of interconnected computers or servers that work together to store and process large volumes of data using the Hadoop framework.\n",
        "Hadoop cluster is a distributed architecture. Hadoop is designed to work across multiple computers or servers (nodes) that are networked together to form a cluster.\n",
        "\n",
        "**Cluster:** Think of it as a team of computers working together, like a group of friends.\n",
        "\n",
        "**Hadoop:** This is the special software that helps these computers collaborate efficiently.\n",
        "\n",
        "**Big Data:** Hadoop clusters are used for handling huge amounts of data, like all the videos on the internet or all the transactions in a bank.\n",
        "\n",
        "In a Hadoop cluster, data is divided into smaller pieces and distributed across the computers.\n",
        "\n",
        "Each computer does a part of the work, and the results are combined, making it possible to handle massive datasets and complex tasks. Hadoop clusters are widely used in businesses and research to analyze, store, and process big data"
      ],
      "metadata": {
        "id": "BXx0e0Z-m4ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C) Datawarehouse Overview:**\n",
        "\n",
        "* Datawarehouse is a giant centralized database that contains information from different sources and ties them together for you\n",
        "\n",
        "* Datawarehouse follows centralized approach so it means they are monolithic. If it doesn't follow centralized approach, then it will be distributed data warehouse.\n",
        "\n",
        "* For example, you work at an e-commerce company which has the following sources of data:\n",
        "\n",
        "  1. Data from Ordering system that feeds information about the stuff people bought\n",
        "  2. Data from web server logs that get ingested into the data warehouse\n",
        "  3. Data from Customer service center that shows relationship between customer and browsing behavior of our company\n",
        "\n",
        "* See, now the data which a datawarehouse is recieving is all in different formats and we cannot apply any query on it to extract any of its data, neither we can get some insights about the data.\n",
        "\n",
        "* It needs to transform this data to some sort of schema on which we can easily apply query.\n",
        "\n",
        "* After transformation, we can easily apply query on it using SQL or using some graphical tool e.g. Tableau\n",
        "\n",
        "(Now a days, data analyst uses tableau to analyse their data. That is the difference in data scientist and data analyst).\n",
        "\n",
        "* Ofcourse, we cannot hire people to transform or handle that much big size of data.\n",
        "\n",
        "**Challenges in Data warehouse:**\n",
        "\n",
        "1. For transformation, we first need to normalize the data. How to figure out that all column are related to each other? Q k normalization me yehi to krty hain sary columns ki values ko aik e scale me le k aty hain by checking correlation of all columns with each other. hmen to pta e ni konsi value kisi dusry column ki value se kesy correlated eh.\n",
        "\n",
        "2. How to deal with missing data, outliers, corrupt data???\n",
        "\n",
        "3. Which is data from robots?\n",
        "\n",
        "4. Scaling is also very difficult when you are dealing with monolithic data warehouse.\n",
        "\n",
        "\n",
        "\n",
        "Bro!!! Bht bari transformation krni pary gi.... Sary data ko utha k aik proper table like form me le k ana k columns compare ho sken. Big deal bro!!!\n",
        "\n",
        "\n",
        "MTLB aik cheez to clear eh bro k data direct to ni rakh skty data warehouse me. isko transform krna pary ga.\n",
        "\n",
        "Ab transform krna kesy eh kry ga kon???\n",
        "\n",
        "So, we need ETL or ELT processes to handle this matter. Yh kya krty hain k data ko bahir e extract kr lety hain aik jaga, phr usko transform kr k phr structured form me load kr dety hain data warehouse me. ya pehly load krdo warehouse me bd me transform krlo. Jese mrzi krlo. Hmen to end of the day transformed data chahye jis ko hm use kr sken.\n",
        "\n",
        "Khair to is trha warehouse me data ajata eh structured form me. or hm us pe query ya tableau se usko analyse kr lety hain.\n",
        "\n",
        "Cool!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mceo5hg0nQvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **D) ETL - Extract, Transform, Load:**\n",
        "\n",
        "* ETL and ELT shows how data gets into the warehouse\n",
        "\n",
        "* The traditional method is ETL.\n",
        "\n",
        "**Steps**:\n",
        "\n",
        "  1. Firstly extract all the data from the operational systems e.g. ordering system, web server log etc. mtlb bahir e kisi cheez medata extract kro in sources se.\n",
        "  2. Now we need to transform it to some structured form like tables. This transformation stage goes through every line of data and then transform that to tables\n",
        "  3. Now load this structured data into data warehouse.\n",
        "\n",
        "Acha g. thk eh. data ko bahir e transform b krlia kisi trha. Ab agr data ki scalability barh jaye?? size bht ziada barh jaye to kesy itnyyyyy bary transformed tables ko load krna eh musibat. mtlb suppose hmary pas log file eh uspe 4 lines likhi hon gi wo load krna asaan eh warehouse me YA un 4 lines ka aik table bnao jis me columns hon k g yh file kab bni kis ne bnai kitni bytes hain is me kya eh kya ni. etc etc. itna bara table enter krna asaan eh??\n",
        "\n",
        "Yh to tnshn eh niri.\n",
        "\n",
        "This problem is solved with a new technique ELT (Extract, load and transform).\n",
        "\n",
        "## **E) ELT - Extract, Load and Transform:**\n",
        "\n",
        "* In ELT (Extract, Load, Transform), the data is first loaded into the data warehouse in its raw or semi-structured form, and the transformation is performed within the warehouse itself. Modern data warehouses, especially those designed for big data, are often capable of handling data transformations efficiently.\n",
        "\n",
        "* Today, using monolithic structure or using single Oracle instance isn't only choice for a large data warehouse.\n",
        "\n",
        "* In the past, using a single Oracle instance (or server) or monolithic structure might have been the standard approach for managing and storing large amounts of data in a data warehouse.\n",
        "\n",
        "* However, with advancements in technology and the availability of various database systems and data warehousing solutions, we can consider using different database technologies, distributed architectures, or cloud-based data warehousing platforms to handle large volumes of data more efficiently and cost-effectively.\n",
        "\n",
        "* Things like Hive/Spark/MapReduce lets you host massive database on a Hadoop cluster.\n",
        "\n",
        "Hive is a data warehousing and SQL-like query language system that works on top of Hadoop. It provides a higher-level, user-friendly interface for interacting with data stored in a Hadoop cluster.\n",
        "\n",
        " Hive is a data warehouse. It helps organize and query large amounts of data stored in a Hadoop cluster, making it easier to analyze and work with that data, similar to how a traditional data warehouse organizes and manages data for analysis.\n",
        "\n",
        "* Things like Hive/Spark/MapReduce let us use the power of distributed databases. These things are used to do the transformation after the loading. This transformation includes tasks like cleaning, aggregating, and structuring the data for analysis.\n",
        "\n",
        "* It means we are gonna use the power of repository itself to do the transformation.\n",
        "\n",
        "* So instead of doing that offline process of transforming line by line to a structured format, we can use hadoop to do the distributed computing for transformation.\n",
        "\n",
        "* So, things like Hive let you host you a large database on a Hadoop cluster.\n",
        "\n",
        "* There are also Spark SQL that let's you do the queries like data warehouse manner on a data warehouse which is on distributed archcitecture like Hadoop Cluster.\n",
        "\n",
        "* There are also NoSQL data stores that can be queried using Spark and MapReduce.\n",
        "\n",
        "Spark and MapReduce, which are big data processing frameworks, can be used to perform queries and data analysis on databases that use a NoSQL (Not Only SQL) data storage approach.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pYPco1IQzk52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------- So the idea is that instead of using some monolithic data warehouse, use something built on top of Hadoop or some cluster that can not only scale up the processing and querying of data but also scale up the transformation of data as well. ----------------\n",
        "\n",
        "\n",
        "So in ELT, you first extract your raw data, then load it in data warehouse and the perform the transformation using power of datawarehouse which is built on top of Hadoop.\n",
        "\n",
        "So, if you move from monolithic database which is built on top of Oracle or MySQL, Intead use distributed databases built on top of Hadoop.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uAJN03sPS__7"
      }
    }
  ]
}